{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b33e0e5b",
   "metadata": {},
   "source": [
    "# bigram model\n",
    "\n",
    "> Let's create a simple bigram model for text generation based on the Tiny Shakespear dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91952496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c067f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe693f77",
   "metadata": {},
   "source": [
    "Add and Initiliaze DVC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6472b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding packages to \u001b[36mdefault\u001b[0m dependencies: \u001b[1;32mdvc\u001b[0m\n",
      "\u001b[2K\u001b[36mâ ¹\u001b[0m Fetching hashes for resolved packages... \u001b[33m1.3.3\u001b[0m[0mm\n",
      "\u001b[1A\u001b[2KðŸ”’ Lock successful\n",
      "Changes are written to \u001b[32mpdm.lock\u001b[0m.\n",
      "Changes are written to \u001b[32mpyproject.toml\u001b[0m.\n",
      "\u001b[2K\u001b[36mâ ¦\u001b[0m Fetching hashes for resolved packages...\n",
      "\u001b[1A\u001b[2KAll packages are synced to date, nothing to do.\n",
      "\u001b[2KInstalling the project as an editable package...\n",
      "\u001b[2K  \u001b[32mâœ”\u001b[0m Update \u001b[1;32mframework-gpt\u001b[0m \u001b[33m0.0.1\u001b[0m -> \u001b[33m0.0.1\u001b[0m successful\n",
      "\u001b[2K36mâ ‡\u001b[0m Updating \u001b[1;32mframework-gpt\u001b[0m \u001b[33m0.0.1\u001b[0m -> \u001b[33m0.0.1\u001b[0m...\n",
      "ðŸŽ‰ All complete!\n",
      "\u001b[2K36mâ ‡\u001b[0m Updating \u001b[1;32mframework-gpt\u001b[0m \u001b[33m0.0.1\u001b[0m -> \u001b[33m0.0.1\u001b[0m...\n",
      "\u001b[?25hInitialized DVC repository.\n",
      "\n",
      "You can now commit the changes to git.\n",
      "\n",
      "\u001b[31m+---------------------------------------------------------------------+\n",
      "\u001b[0m\u001b[31m|\u001b[0m                                                                     \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m        DVC has enabled anonymous aggregate usage analytics.         \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m     Read the analytics documentation (and how to opt-out) here:     \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m             <\u001b[36mhttps://dvc.org/doc/user-guide/analytics\u001b[39m>              \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m                                                                     \u001b[31m|\u001b[0m\n",
      "\u001b[31m+---------------------------------------------------------------------+\n",
      "\u001b[0m\n",
      "\u001b[33mWhat's next?\u001b[39m\n",
      "\u001b[33m------------\u001b[39m\n",
      "- Check out the documentation: <\u001b[36mhttps://dvc.org/doc\u001b[39m>\n",
      "- Get help and share ideas: <\u001b[36mhttps://dvc.org/chat\u001b[39m>\n",
      "- Star us on GitHub: <\u001b[36mhttps://github.com/iterative/dvc\u001b[39m>\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pdm add dvc\n",
    "!pdm run dvc init --subdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3573d46b",
   "metadata": {},
   "source": [
    "We will be using PyTorch for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaebde7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a28c85a",
   "metadata": {},
   "source": [
    "Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f56f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32 # how many sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # use cuda if available\n",
    "eval_iters = 200\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc25f5b6",
   "metadata": {},
   "source": [
    "Getting the Tiny Shakepeare dataset (a small text file containing all the works of Shakepeare):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d6dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O /home/nclark/git/framework_gpt/nbs/data/raw/input.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "path = '/home/nclark/git/framework_gpt/nbs/data/raw/input.txt'\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c97bfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR\u001b[39m: Stage 'raw_data' already exists in 'dvc.yaml'. Use '--force' to overwrite.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pdm run dvc stage add -n raw_data \\\n",
    "    -d 00_bigram_dvc.ipynb \\\n",
    "    -d pyproject.toml \\\n",
    "    -d pdm.lock \\\n",
    "    python 00_bigram_dvc.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db090735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR\u001b[39m: '/home/nclark/git/framework_gpt/nbs/00_bigram_dvc.ipynb' is specified as an output and as a dependency.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pdm run dvc stage add -f \\\n",
    "    -n raw_data \\\n",
    "    -d 00_bigram_dvc.ipynb \\\n",
    "    -d pyproject.toml \\\n",
    "    -d pdm.lock \\\n",
    "    -d data \\\n",
    "    -o 00_bigram_dvc.ipynb \\\n",
    "    python 00_bigram_dvc.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963b6351",
   "metadata": {},
   "source": [
    "Vocabulary Creation and Encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d46d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] #encoder: string -> list of integers\n",
    "decode = lambda l: ''.join(itos[i] for i in l) #decoder: list of integers -> string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739265c7",
   "metadata": {},
   "source": [
    "90/10 split into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853d4122",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c5699",
   "metadata": {},
   "source": [
    "Data Loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329554fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data =train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e446d3",
   "metadata": {},
   "source": [
    "Create an estimate_loss function for evaluation while training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cabb004",
   "metadata": {},
   "source": [
    "Bigram Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a912c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B, T) tensors of integers\n",
    "        logits = self.token_embedding_table(idx) #(B, T, C)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predicions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B,C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=1) # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6587c1",
   "metadata": {},
   "source": [
    "Initialize Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc0f60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272b0d9e",
   "metadata": {},
   "source": [
    "Optimizer and Training Loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e6d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.6742, val loss 4.6920\n",
      "step 1000: train loss 2.5033, val loss 2.5159\n",
      "step 2000: train loss 2.4623, val loss 2.4900\n",
      "step 3000: train loss 2.4474, val loss 2.4964\n",
      "step 4000: train loss 2.4567, val loss 2.4877\n",
      "step 5000: train loss 2.4525, val loss 2.4839\n",
      "step 6000: train loss 2.4619, val loss 2.4895\n",
      "step 7000: train loss 2.4466, val loss 2.4886\n",
      "step 8000: train loss 2.4475, val loss 2.4915\n",
      "step 9000: train loss 2.4525, val loss 2.4962\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    \n",
    "    # evaluate the loss on train and val sets every eval_interval\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    #evluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44e3ffb",
   "metadata": {},
   "source": [
    "Generate Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6204034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LUCEThinge Bulkisstone Sche,\n",
      "The we d,\n",
      "\n",
      "Anomed weron\n",
      "Fiurwataltho tororent have t,'ld tyou to ard an; he: gesily he t-hravo,\n",
      "Ang, heancour inowisseeay.\n",
      "\n",
      "ARoubesirnd y lorge.\n",
      "WI mee KERin t.\n",
      "BI che igserd't or!\n",
      "Weas bed piris we r n foo Y n h\n",
      "HARDUn t, ur, Be: cther s: at, I RLISee, yp ming thed wast. shabrwowhee\n",
      "Mayowirsafod polld ORIINT: handsth hir yot'digr t mo hark!\n",
      "H:\n",
      "And, neadit! orowie th s he istos\n",
      "lof avomase RI'd TE m rur ak-\n",
      "Wheriret s peco br IN: hwomavir\n",
      "BAs ifabearthor f ad.\n",
      "Thirer\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
